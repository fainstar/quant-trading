from __future__ import annotations

import argparse
import json
import logging
from collections import defaultdict
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Dict, Iterable, List, Tuple

import numpy as np
import pandas as pd
from sklearn.metrics import accuracy_score, classification_report

logger = logging.getLogger(__name__)


MODEL_NAME_ZH = 'Ê®∏Á¥†Ë≤ùËëâÊñØ'
MODEL_NAME_EN = 'Categorical Naive Bayes'

# ‰∫§ÊòìË®äËôüÁ≠ñÁï•ÂèÉÊï∏
FUTURE_DAYS = 5
PCT_CHANGE_THRESHOLD = 0.03

def define_signal(price_change: float) -> str:
    """
    Ê†πÊìöÂÉπÊ†ºËÆäÂåñÂÆöÁæ©‰∫§ÊòìË®äËôü
    
    Args:
        price_change: ÂÉπÊ†ºËÆäÂåñÁôæÂàÜÊØî
        
    Returns:
        str: ‰∫§ÊòìË®äËôü - 'Ë≤∑ÂÖ•', 'Ë≥£Âá∫' Êàñ 'ÊåÅÊúâ'
    """
    if price_change > PCT_CHANGE_THRESHOLD:
        return 'Ë≤∑ÂÖ•'
    elif price_change < -PCT_CHANGE_THRESHOLD:
        return 'Ë≥£Âá∫'
    else:
        return 'ÊåÅÊúâ'


class NaiveBayesClassifier:
    """Á∞°ÂñÆÁöÑÈ°ûÂà•ÂûãÊ®∏Á¥†Ë≤ùËëâÊñØÂàÜÈ°ûÂô®ÔºåÊîØÊè¥ÊãâÊôÆÊãâÊñØÂπ≥Êªë„ÄÇ"""

    def __init__(self, laplace_smoothing: float = 1.0) -> None:
        if laplace_smoothing <= 0:
            raise ValueError('laplace_smoothing ÂøÖÈ†àÁÇ∫Ê≠£Êï∏')
        self.laplace_smoothing = float(laplace_smoothing)
        self.priors: Dict[int, float] = {}
        self.likelihoods: dict[str, dict[int, dict[float, float]]] = defaultdict(lambda: defaultdict(dict))
        self.features: list[str] = []
        self.classes_: list[int] = []

    def fit(self, X: pd.DataFrame, y: pd.Series) -> 'NaiveBayesClassifier':
        if len(X) != len(y):
            raise ValueError('ÁâπÂæµËàáÊ®ôÁ±§Èï∑Â∫¶‰∏ç‰∏ÄËá¥')

        y = pd.Series(y).astype(int)
        self.features = list(X.columns)
        # Á¢∫‰øùÈ°ûÂà•ÂåÖÂê´ -1, 0, 1ÔºåÂç≥‰ΩøÊï∏Êìö‰∏≠ÂèØËÉΩÊö´ÊôÇÊ≤íÊúâ
        # ÈÄôÂ∞çÊñº‰∏âÊÖã‰∫§ÊòìË®äËôüÔºàË≤∑ÂÖ•„ÄÅÊåÅÊúâ„ÄÅË≥£Âá∫ÔºâÂæàÈáçË¶Å
        unique_classes = sorted(y.unique().tolist())
        self.classes_ = sorted(list(set(unique_classes + [-1, 0, 1])))
        n_samples = len(y)

        if not self.classes_:
            raise ValueError('Ë®ìÁ∑¥Ë≥áÊñô‰∏≠Ê≤íÊúâ‰ªª‰ΩïÈ°ûÂà•Ê®ôÁ±§')

        for action in self.classes_:
            action_count = int((y == action).sum())
            self.priors[action] = (action_count + self.laplace_smoothing) / (
                n_samples + self.laplace_smoothing * len(self.classes_)
            )

        for feature in self.features:
            feature_series = X[feature]
            unique_values = pd.Series(feature_series.unique()).dropna().tolist()
            if not unique_values:
                unique_values = [0.0]
            num_unique = len(unique_values)

            for action in self.classes_:
                action_mask = y == action
                feature_values_in_action = feature_series[action_mask]
                value_counts = feature_values_in_action.value_counts()
                total_in_action = len(feature_values_in_action)

                denominator = total_in_action + num_unique * self.laplace_smoothing
                if denominator == 0:
                    denominator = num_unique * self.laplace_smoothing

                for value in unique_values:
                    value_key = float(value)
                    count = value_counts.get(value, 0)
                    probability = (count + self.laplace_smoothing) / denominator
                    self.likelihoods[feature][action][value_key] = probability

                # Ëã•ÈÅáÂà∞Êú™Ë¶ãÂÄºÔºåÁµ¶‰∫àÊ•µÂ∞èÂπ≥ÊªëÊ©üÁéáÂÇôÁî®
                self.likelihoods[feature][action]['__unknown__'] = self.laplace_smoothing / (
                    denominator if denominator > 0 else self.laplace_smoothing * num_unique
                )

        return self

    def _compute_log_posteriors(self, row: pd.Series) -> Dict[int, float]:
        posteriors: Dict[int, float] = {}
        for action in self.classes_:
            log_posterior = float(np.log(self.priors.get(action, 1e-12)))
            for feature in self.features:
                value_key = float(row.get(feature, 0.0))
                feature_likelihoods = self.likelihoods.get(feature, {}).get(action, {})
                probability = feature_likelihoods.get(value_key)
                if probability is None or probability <= 0:
                    probability = feature_likelihoods.get('__unknown__', 1e-12)
                log_posterior += float(np.log(probability))
            posteriors[action] = log_posterior
        return posteriors

    def predict_log_proba(self, X: pd.DataFrame) -> np.ndarray:
        log_probas: list[list[float]] = []
        for _, row in X.iterrows():
            posteriors = self._compute_log_posteriors(row)
            log_probas.append([posteriors[action] for action in self.classes_])
        return np.array(log_probas)

    def predict_proba(self, X: pd.DataFrame) -> np.ndarray:
        log_probas = self.predict_log_proba(X)
        # ‰ΩøÁî® log-sum-exp ËΩâÊèõÁÇ∫ÂØ¶ÈöõÊ©üÁéá
        max_log = log_probas.max(axis=1, keepdims=True)
        stabilized = np.exp(log_probas - max_log)
        sums = stabilized.sum(axis=1, keepdims=True)
        with np.errstate(divide='ignore', invalid='ignore'):
            probas = np.divide(stabilized, sums, where=sums > 0)
        return probas

    def predict(self, X: pd.DataFrame) -> np.ndarray:
        probas = self.predict_proba(X)
        class_indices = np.argmax(probas, axis=1)
        return np.array([self.classes_[idx] for idx in class_indices])


@dataclass
class PredictionConfig:
    input_path: Path = Path('data/final_data.csv')
    model_output_path: Path = Path('data/trained_model.pkl')
    report_output_path: Path = Path('log/daily_trading_report.txt')
    report_metadata_path: Path = Path('log/daily_trading_report.json')
    test_size: float = 0.2
    random_state: int = 42
    rolling_window_size: int = 500


def load_final_data(path: Path) -> pd.DataFrame:
    if not path.exists():
        raise FileNotFoundError(f"Êâæ‰∏çÂà∞ÊúÄÁµÇÊï¥ÁêÜË≥áÊñôÊ™î: {path}")
    logger.info("ËºâÂÖ•Êï¥ÁêÜÂæåË≥áÊñô: %s", path)
    return pd.read_csv(path)


def create_labels(data: pd.DataFrame) -> pd.Series:
    if 'Close' not in data.columns:
        raise ValueError("Ë≥áÊñô‰∏≠Áº∫Â∞ë Close Ê¨Ñ‰ΩçÔºåÁÑ°Ê≥ïÁîüÊàêÊ®ôÁ±§")
    
    # ‰ΩøÁî®ÂÖ®Â±ÄËÆäÈáèÂÆöÁæ©ÁöÑ‰∫§ÊòìË®äËôüÁ≠ñÁï•ÂèÉÊï∏
    # Ë®àÁÆóÊú™‰æÜÂÉπÊ†ºËÆäÂãï
    future_close = data['Close'].shift(-FUTURE_DAYS)
    price_change = (future_close - data['Close']) / data['Close']
    
    # ‰ΩøÁî® define_signal ÂáΩÊï∏Â∞áÂÉπÊ†ºËÆäÂåñÊò†Â∞ÑÁÇ∫‰∫§ÊòìË®äËôü
    # ÁÑ∂ÂæåÂ∞áÊñáÊú¨Ë®äËôüËΩâÊèõÁÇ∫Êï∏ÂÄºÔºö1=Ë≤∑ÂÖ•, -1=Ë≥£Âá∫, 0=ÊåÅÊúâ
    signal_mapping = {'Ë≤∑ÂÖ•': 1, 'Ë≥£Âá∫': -1, 'ÊåÅÊúâ': 0}
    labels = price_change.apply(define_signal).map(signal_mapping)
    
    # Âõ†ÁÇ∫‰ΩøÁî®‰∫Üfuture_daysÂ§©ÁöÑÊú™‰æÜÊï∏ÊìöÔºåÈúÄË¶ÅË£ÅÂâ™Áõ∏ÊáâÁöÑÂ∞æÈÉ®Êï∏Êìö
    return pd.Series(labels, name='next_day_signal').iloc[:-FUTURE_DAYS]


def prepare_features(data: pd.DataFrame) -> Tuple[pd.DataFrame, pd.Series]:
    logger.info("Ê∫ñÂÇôÁâπÂæµËàáÊ®ôÁ±§...")
    label_series = create_labels(data)
    # Ê†πÊìöÊ®ôÁ±§Èï∑Â∫¶Ë™øÊï¥ÁâπÂæµÈï∑Â∫¶ÔºåÁ¢∫‰øùÁâπÂæµËàáÊ®ôÁ±§Èï∑Â∫¶‰∏ÄËá¥
    feature_frame = data.iloc[:-FUTURE_DAYS].drop(columns=['Date', 'Ticker'], errors='ignore')
    encoded_features = pd.get_dummies(feature_frame, dummy_na=False)
    encoded_features = encoded_features.fillna(0)
    return encoded_features, label_series


def train_model(X_train: pd.DataFrame, y_train: pd.Series) -> NaiveBayesClassifier:
    logger.info("Ë®ìÁ∑¥ %s (%s) Ê®°Âûã...", MODEL_NAME_ZH, MODEL_NAME_EN)
    model = NaiveBayesClassifier()
    model.fit(X_train, y_train)
    return model


def rolling_train_and_evaluate(
    features: pd.DataFrame,
    labels: pd.Series,
    window_size: int
) -> Tuple[NaiveBayesClassifier, Dict[str, float | str | List[int] | List[float]]]:
    if window_size <= 0:
        raise ValueError('ÊªæÂãïÁ™óÂè£Â§ßÂ∞èÂøÖÈ†àÁÇ∫Ê≠£Êï¥Êï∏')

    total_samples = len(features)
    if total_samples <= 1:
        raise ValueError('ÂèØÁî®Ê®£Êú¨Êï∏‰∏çË∂≥ÔºåËá≥Â∞ëÈúÄË¶Å 2 Á≠ÜË≥áÊñô')

    if total_samples < window_size:
        logger.warning(
            "ÂèØÁî®Ê®£Êú¨Êï∏ (%d) Â∞èÊñºË®≠ÂÆöÁöÑÊªæÂãïÁ™óÂè£Â§ßÂ∞è (%d)ÔºåÂ∞áÊîπÁî® %d Á≠ÜË≥áÊñôË¶ñÁ™ó",
            total_samples,
            window_size,
            total_samples - 1,
        )
        window_size = total_samples - 1

    predictions: List[int] = []
    actuals: List[int] = []
    probabilities: List[float] = []
    model: NaiveBayesClassifier | None = None

    logger.info("‰ª• %d Á≠ÜË≥áÊñôÁ™óÂè£Âü∑Ë°åÊØèÊó•ÊªæÂãïË®ìÁ∑¥...", window_size)
    for idx in range(window_size, total_samples):
        X_train = features.iloc[idx - window_size:idx]
        y_train = labels.iloc[idx - window_size:idx]
        model = train_model(X_train, y_train)

        X_test = features.iloc[idx:idx + 1]
        y_true = int(labels.iloc[idx])
        y_pred = int(model.predict(X_test)[0])
        predictions.append(y_pred)
        actuals.append(y_true)

        if hasattr(model, 'predict_proba') and len(model.classes_) > 1:
            # Áç≤ÂèñÈ†êÊ∏¨È°ûÂà•ÁöÑÊ¶ÇÁéá‰ΩúÁÇ∫‰ø°ÂøÉÂÄº
            proba_dist = model.predict_proba(X_test)[0]
            predicted_class_idx = np.argmax(proba_dist)
            probability = float(proba_dist[predicted_class_idx])
        else:
            probability = 1.0 if y_pred == y_true else 0.0
        probabilities.append(probability)

    if model is None:
        # Ëã•Ê≤íÊúâÁî¢Áîü‰ªª‰ΩïÊ∏¨Ë©¶Èªû (Ë≥áÊñôÈáèÊÅ∞ÁÇ∫Á™óÂè£Â§ßÂ∞è)ÔºåËá≥Â∞ëË®ìÁ∑¥Âá∫ÊúÄÂæåÊ®°Âûã
        X_train = features.iloc[-window_size:]
        y_train = labels.iloc[-window_size:]
        model = train_model(X_train, y_train)

    accuracy = accuracy_score(actuals, predictions) if actuals else 0.0
    report = (
        classification_report(actuals, predictions, zero_division=0)
        if actuals
        else 'Ê®£Êú¨‰∏çË∂≥ÔºåÁÑ°Ê≥ïÁîüÊàêÊªæÂãïÈ©óË≠âÂ†±Âëä'
    )

    evaluation = {
        'accuracy': accuracy,
        'classification_report': report,
        'rolling_predictions': predictions,
        'rolling_actuals': actuals,
        'rolling_probabilities': probabilities,
    }

    return model, evaluation


def prepare_latest_features(latest_row: pd.Series, feature_columns: List[str]) -> pd.DataFrame:
    latest_features = latest_row.drop(labels=['Date', 'Ticker'], errors='ignore')
    latest_df = pd.DataFrame([latest_features])
    latest_encoded = pd.get_dummies(latest_df, dummy_na=False)
    latest_encoded = latest_encoded.reindex(columns=feature_columns, fill_value=0)
    latest_encoded = latest_encoded.fillna(0)
    return latest_encoded


def generate_trading_signal(model: NaiveBayesClassifier, latest_features: pd.DataFrame) -> Tuple[int, float, Dict[int, float]]:
    """
    Ê†πÊìöÊ®°ÂûãÁîüÊàê‰∫§ÊòìË®äËôüÂíå‰ø°ÂøÉÂÄº
    
    Args:
        model: Ë®ìÁ∑¥Â•ΩÁöÑÂàÜÈ°ûÊ®°Âûã
        latest_features: ÊúÄÊñ∞ÁöÑÁâπÂæµÊï∏Êìö
        
    Returns:
        Tuple: ÂåÖÂê´‰∏âÈ†ÖÂÖÉÁ¥†
            - int: ‰∫§ÊòìË®äËôüÔºà1=Ë≤∑ÂÖ•, -1=Ë≥£Âá∫, 0=ÊåÅÊúâÔºâ
            - float: ‰ø°ÂøÉÂÄºÔºàÊúÄÈ´òÈ°ûÂà•ÁöÑÊ¶ÇÁéáÔºâ
            - Dict: ÂêÑÈ°ûÂà•ÁöÑÊ¶ÇÁéáÂàÜÂ∏É
    """
    prediction = model.predict(latest_features)[0]
    # Áç≤ÂèñÊâÄÊúâÈ°ûÂà•ÁöÑÊ¶ÇÁéáÂàÜÂ∏É
    proba_dist = model.predict_proba(latest_features)[0]
    # ÂâµÂª∫È°ûÂà•ËàáÊ¶ÇÁéáÁöÑÊò†Â∞Ñ
    class_probs = {cls: float(prob) for cls, prob in zip(model.classes_, proba_dist)}
    # Áç≤ÂèñÊúÄÈ´òÊ¶ÇÁéáÂÄº‰ΩúÁÇ∫‰ø°ÂøÉÂÄº
    probability = float(proba_dist.max())
    
    # ‰∏âÊÖã‰∫§ÊòìË®äËôüÔºö1=Ë≤∑ÂÖ•, -1=Ë≥£Âá∫, 0=ÊåÅÊúâ
    signal = int(prediction)
    
    return signal, probability, class_probs


def format_report(
    evaluation: Dict[str, float | str],
    signal: int,
    probability: float,
    latest_price: float,
    latest_date: str,
    ticker: str,
    generated_at: str,
    class_probs: Dict[int, float] = None
) -> str:
    # Ê†πÊìöË®äËôüÁ¢∫ÂÆöÂª∫Ë≠∞ÂíåË°®ÊÉÖÁ¨¶Ëôü
    if signal == 1:
        recommendation = 'Ë≤∑ÈÄ≤'
        signal_emoji = 'üü¢'
        signal_description = f"È†êÊúüÊú™‰æÜ{FUTURE_DAYS}Â§©ÂÖßÂÉπÊ†º‰∏äÊº≤Ë∂ÖÈÅé{PCT_CHANGE_THRESHOLD*100}%"
    elif signal == -1:
        recommendation = 'Ë≥£Âá∫'
        signal_emoji = 'üî¥'
        signal_description = f"È†êÊúüÊú™‰æÜ{FUTURE_DAYS}Â§©ÂÖßÂÉπÊ†º‰∏ãË∑åË∂ÖÈÅé{PCT_CHANGE_THRESHOLD*100}%"
    else:  # signal == 0
        recommendation = 'ÊåÅÊúâ'
        signal_emoji = '‚ö™'
        signal_description = f"È†êÊúüÊú™‰æÜ{FUTURE_DAYS}Â§©ÂÖßÂÉπÊ†ºÊ≥¢Âãï‰∏çË∂ÖÈÅé{PCT_CHANGE_THRESHOLD*100}%"

    # ‰ø°ÂøÉÂÄºË©≥Á¥∞Ë™™Êòé
    confidence_section = f"üéØ ‰ø°ÂøÉÂÄºÔºö{probability:.2%}"
    
    # Â¶ÇÊûúÊèê‰æõ‰∫ÜË©≥Á¥∞ÁöÑÈ°ûÂà•Ê¶ÇÁéáÔºåÊ∑ªÂä†Ê¶ÇÁéáÂàÜÂ∏É‰ø°ÊÅØ
    if class_probs:
        signal_mapping = {1: 'Ë≤∑ÈÄ≤', -1: 'Ë≥£Âá∫', 0: 'ÊåÅÊúâ'}
        probs_text = []
        for cls in sorted(class_probs.keys(), reverse=True):
            if cls in signal_mapping:
                probs_text.append(f"  {signal_mapping[cls]}: {class_probs[cls]:.2%}")
        
        if probs_text:
            confidence_section += "\nüìä Ê¶ÇÁéáÂàÜÂ∏ÉÔºö\n" + "\n".join(probs_text)

    sections = [
        f"{signal_emoji} ÊØèÊó•ÈáèÂåñ‰∫§ÊòìÂ†±Âëä - {ticker}",
        f"üìä {MODEL_NAME_ZH}Ê®°ÂûãÈáèÂåñÂàÜÊûêÁµêÊûú",
        f"üè¢ ËÇ°Á•®ÂêçÁ®±Ôºö{ticker}",
        f"üïí Â†±ÂëäÁîüÊàêÊôÇÈñìÔºö{generated_at}",
        f"üíµ Êî∂Áõ§ÂÉπÔΩú{latest_price:,.2f}",
        f"üìà Âª∫Ë≠∞Êìç‰ΩúÔΩú{recommendation}",
        f"üîÆ Ë®äËôüË™™ÊòéÔΩú{signal_description}",
        confidence_section,
        "",
        "‚Äî‚Äî Ê®°ÂûãË©ï‰º∞ÊëòË¶Å ‚Äî‚Äî",
        f"AccuracyÔºö{evaluation['accuracy']:.2%}"
    ]

    sections.append("‚ö†Ô∏è Êú¨Â†±ÂëäÁÇ∫ÈáèÂåñÊ®°ÂûãÂàÜÊûêÁµêÊûúÔºåÈùûÊäïË≥áÂª∫Ë≠∞„ÄÇË´ãÂØ©ÊÖéË©ï‰º∞È¢®Èö™„ÄÇ")

    return "\n".join(section for section in sections if section is not None)


def save_report(report: str, path: Path) -> Path:
    path.parent.mkdir(parents=True, exist_ok=True)
    path.write_text(report, encoding='utf-8')
    logger.info("Â†±ÂëäÂ∑≤ÂÑ≤Â≠òËá≥: %s", path)
    return path


def save_report_metadata(metadata: Dict[str, str | float | int], path: Path) -> Path:
    path.parent.mkdir(parents=True, exist_ok=True)
    path.write_text(json.dumps(metadata, ensure_ascii=False, indent=2), encoding='utf-8')
    logger.info("Â†±Âëä‰∏≠ÁπºË≥áÊñôÂ∑≤ÂÑ≤Â≠òËá≥: %s", path)
    return path


def run_prediction_pipeline(config: PredictionConfig) -> Dict[str, str | float | int]:
    data = load_final_data(config.input_path)
    features, labels = prepare_features(data)
    feature_columns = features.columns.tolist()
    model, evaluation = rolling_train_and_evaluate(features, labels, config.rolling_window_size)
    logger.info(
        "ÊªæÂãïÈ©óË≠âÂÆåÊàêÔºöÂÖ± %d Á≠ÜÊ®£Êú¨Â§ñÈ†êÊ∏¨ÔºåÊ∫ñÁ¢∫Áéá %.2f%%",
        len(evaluation['rolling_predictions']),
        evaluation['accuracy'] * 100,
    )
    logger.info("‰ΩøÁî® %d Â§©Êú™‰æÜË¶ñÁ™óÔºå%.2f%% ÂÉπÊ†ºËÆäÂåñÈñæÂÄºÁöÑ‰∫§ÊòìË®äËôüÁ≠ñÁï•", FUTURE_DAYS, PCT_CHANGE_THRESHOLD*100)

    latest_row = data.iloc[-1]
    latest_date = latest_row.get('Date', 'Êú™Áü•Êó•Êúü')
    latest_price = float(latest_row.get('Close', float('nan')))
    ticker = latest_row.get('Ticker', 'Êú™Áü•Ê®ôÁöÑ')
    latest_features = prepare_latest_features(latest_row, feature_columns)
    signal, probability, class_probs = generate_trading_signal(model, latest_features)

    generated_at = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

    report_text = format_report(
        evaluation=evaluation,
        signal=signal,
        probability=probability,
        latest_price=latest_price,
        latest_date=latest_date,
        ticker=ticker,
        generated_at=generated_at,
        class_probs=class_probs,
    )
    save_report(report_text, config.report_output_path)

    # Ê†πÊìöË®äËôüÁ¢∫ÂÆöÂª∫Ë≠∞
    if signal == 1:
        recommendation = 'Ë≤∑ÈÄ≤'
    elif signal == -1:
        recommendation = 'Ë≥£Âá∫'
    else:  # signal == 0
        recommendation = 'ÊåÅÊúâ'
    
    # Ê∫ñÂÇôÈ°ûÂà•Ê¶ÇÁéáÁöÑÂ≠óÁ¨¶‰∏≤Ë°®Á§∫ÔºåÁî®ÊñºÂÖÉÊï∏Êìö
    class_probs_dict = {str(k): v for k, v in class_probs.items()}
    
    report_metadata = {
        'ticker': ticker,
        'generated_at': generated_at,
        'latest_date': latest_date,
        'latest_price': latest_price,
        'signal': signal,
        'signal_text': recommendation,  # Ê∑ªÂä†ÊñáÂ≠óË®äËôü
        'recommendation': recommendation,
        'probability': probability,
        'class_probabilities': class_probs_dict,  # Ê∑ªÂä†È°ûÂà•Ê¶ÇÁéáÂàÜÂ∏É
        'accuracy': evaluation['accuracy'],
        'rolling_window_size': config.rolling_window_size,
        'future_days': FUTURE_DAYS,
        'pct_change_threshold': PCT_CHANGE_THRESHOLD,
        'model_name': MODEL_NAME_ZH,
        'model_name_en': MODEL_NAME_EN,
    }
    save_report_metadata(report_metadata, config.report_metadata_path)

    if signal == 1:
        recommendation_text = 'Ë≤∑ÈÄ≤'
    elif signal == -1:
        recommendation_text = 'Ë≥£Âá∫'
    else:  # signal == 0
        recommendation_text = 'ÊåÅÊúâ'
    logger.info("Áï∂Êó•Âª∫Ë≠∞: %s", recommendation_text)

    return {
        'signal': signal,
        'probability': probability,
        'class_probabilities': class_probs,  # Ê∑ªÂä†È°ûÂà•Ê¶ÇÁéáÂàÜÂ∏É
        'accuracy': evaluation['accuracy'],
        'classification_report': evaluation['classification_report'],
        'report': report_text,
        'latest_price': latest_price,
        'latest_date': latest_date,
        'ticker': ticker,
        'generated_at': generated_at,
        'report_metadata_path': str(config.report_metadata_path),
        'rolling_window_size': config.rolling_window_size,
        'rolling_prediction_count': len(evaluation['rolling_predictions']),
        'model_name': MODEL_NAME_ZH,
        'model_name_en': MODEL_NAME_EN,
    }


def build_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(description="Ë®ìÁ∑¥Ê®°Âûã‰∏¶ÁîüÊàêÈöîÊó•‰∫§ÊòìË®äËôü")
    parser.add_argument('--input', type=Path, default=PredictionConfig.input_path, help='Êï¥ÁêÜÂæåË≥áÊñôËº∏ÂÖ•Ê™î')
    parser.add_argument('--model-output', type=Path, default=PredictionConfig.model_output_path, help='Ê®°ÂûãËº∏Âá∫Ë∑ØÂæë (È†êÁïô)')
    parser.add_argument('--report-output', type=Path, default=PredictionConfig.report_output_path, help='Êó•Â†±Ëº∏Âá∫Ë∑ØÂæë')
    parser.add_argument('--metadata-output', type=Path, default=PredictionConfig.report_metadata_path, help='Êó•Â†±‰∏≠ÁπºË≥áÊñôËº∏Âá∫Ë∑ØÂæë')
    parser.add_argument('--test-size', type=float, default=PredictionConfig.test_size, help='Ê∏¨Ë©¶ÈõÜÊØî‰æã')
    parser.add_argument('--random-state', type=int, default=PredictionConfig.random_state, help='Èö®Ê©üÁ®ÆÂ≠ê')
    parser.add_argument('--rolling-window-size', type=int, default=PredictionConfig.rolling_window_size, help='ÊªæÂãïË®ìÁ∑¥Á™óÂè£Â§ßÂ∞è')
    parser.add_argument('--log-level', default='INFO', choices=['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'], help='Êó•Ë™åÂ±§Á¥ö')
    return parser


def main(argv: Iterable[str] | None = None) -> int:
    parser = build_parser()
    args = parser.parse_args(list(argv) if argv is not None else None)

    logging.basicConfig(level=getattr(logging, args.log_level))

    config = PredictionConfig(
        input_path=args.input,
        model_output_path=args.model_output,
        report_output_path=args.report_output,
        report_metadata_path=args.metadata_output,
        test_size=args.test_size,
        random_state=args.random_state,
        rolling_window_size=args.rolling_window_size,
    )

    try:
        result = run_prediction_pipeline(config)
    except (FileNotFoundError, ValueError) as exc:
        logger.error("%s", exc)
        return 1

    logger.info("Êìç‰ΩúÂÆåÊàê„ÄÇÊ®°ÂûãÊ∫ñÁ¢∫Áéá: %.2f%%", result['accuracy'] * 100)
    return 0


__all__ = [
    'PredictionConfig',
    'run_prediction_pipeline',
    'main',
    'FUTURE_DAYS',
    'PCT_CHANGE_THRESHOLD',
    'define_signal',
]
